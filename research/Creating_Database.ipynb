{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Contabot Database\n",
    "\n",
    "In order to create a integrated bot with a enterprise database, we will use some mock data available in [this repository](https://github.com/sinjoysaha/sales-analysis) (credits to [@sinjoysaha](https://github.com/sinjoysaha)) to simulate a history of sales.\n",
    "\n",
    "## Steps\n",
    "1. Clone the repository\n",
    "2. Read the CSV files\n",
    "3. Load all these files to a pandas dataframe\n",
    "    a. Remove empty examples\n",
    "4. Merge the dataframes\n",
    "5. Save the examples to an unique dataframe\n",
    "6. Save a new CSV file with all the examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "REPOSITORY_DATASET_PATH = os.path.join(CURRENT_PATH, 'sales-analysis', 'all_data.csv')\n",
    "OUTPUT_ORDERS_PATH = os.path.join(CURRENT_PATH, 'output-orders.csv')\n",
    "OUTPUT_PRODUCTS_PATH = os.path.join(CURRENT_PATH, 'output-products.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'sales-analysis' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sinjoysaha/sales-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dataset = pd.read_csv(REPOSITORY_DATASET_PATH, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing incorrect rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 186495 entries, 0 to 186849\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Order ID          185950 non-null  object\n",
      " 1   Product           185950 non-null  object\n",
      " 2   Quantity Ordered  185950 non-null  object\n",
      " 3   Price Each        185950 non-null  object\n",
      " 4   Order Date        185950 non-null  object\n",
      " 5   Purchase Address  185950 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "incorrect_samples_dataframe = sales_dataset[sales_dataset['Product'] == 'Product']\n",
    "sales_dataset = sales_dataset.drop(incorrect_samples_dataframe.index)\n",
    "sales_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185950 entries, 0 to 186849\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Product           185950 non-null  object\n",
      " 1   Quantity Ordered  185950 non-null  object\n",
      " 2   Price Each        185950 non-null  object\n",
      " 3   Order Date        185950 non-null  object\n",
      " 4   Purchase Address  185950 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "sales_dataset = sales_dataset.dropna()\n",
    "sales_dataset = sales_dataset.drop(columns=['Order ID'])\n",
    "sales_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset just with the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19 entries, 0 to 109\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Product  19 non-null     object\n",
      " 1   Price    19 non-null     object\n",
      " 2   ID       19 non-null     int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 608.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "product_dataset = sales_dataset.drop_duplicates(subset=['Product'])\n",
    "product_dataset = product_dataset.drop(columns=['Order Date', 'Purchase Address', 'Quantity Ordered'])\n",
    "product_dataset = product_dataset.rename({'Price Each': 'Price'}, axis='columns')\n",
    "product_dataset['ID'] = range(len(product_dataset))\n",
    "product_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching each order with its respective product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dataset = product_dataset.set_index('Product', drop=False)\n",
    "products_as_dict = product_dataset.to_dict('index')\n",
    "\n",
    "def match_product_order(x):\n",
    "    id = products_as_dict[x['Product']]\n",
    "    x['Product ID'] = id['ID']\n",
    "    return x\n",
    "\n",
    "sales_dataset['Product ID'] = 0\n",
    "sales_dataset = sales_dataset.apply(match_product_order, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing remaining columns that will be not used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dataset = sales_dataset.drop(columns=['Product', 'Price Each', 'Purchase Address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dataset.to_csv(OUTPUT_ORDERS_PATH, sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dataset.to_csv(OUTPUT_PRODUCTS_PATH, sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alura",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
